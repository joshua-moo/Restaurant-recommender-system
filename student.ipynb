{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147f7877",
   "metadata": {},
   "source": [
    "## Dining Guide: Personalized Restaurant Recommendations\n",
    "\n",
    "Group Members\n",
    "\n",
    "* Wendy Muturi\n",
    "* Mulei Mutuku\n",
    "* Margaret Mitey\n",
    "* Jeff Kiarie\n",
    "* Linus Gichuhi\n",
    "* Joshua Ooko\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47611f67",
   "metadata": {},
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dining-Guide:-Personalized-Restaurant-Recommendations\" data-toc-modified-id=\"Dining-Guide:-Personalized-Restaurant-Recommendations-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Dining Guide: Personalized Restaurant Recommendations</a></span></li><li><span><a href=\"#Business-Understanding\" data-toc-modified-id=\"Business-Understanding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Business Understanding</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Introduction</a></span></li><li><span><a href=\"#Problem-Statement\" data-toc-modified-id=\"Problem-Statement-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Problem Statement</a></span></li><li><span><a href=\"#Main-Objective\" data-toc-modified-id=\"Main-Objective-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Main Objective</a></span></li><li><span><a href=\"#Specific-Objective\" data-toc-modified-id=\"Specific-Objective-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>Specific Objective</a></span></li><li><span><a href=\"#Metric-of-Success\" data-toc-modified-id=\"Metric-of-Success-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Metric of Success</a></span></li></ul></li><li><span><a href=\"#Data-Understanding\" data-toc-modified-id=\"Data-Understanding-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Understanding</a></span></li><li><span><a href=\"#Data-Preparation\" data-toc-modified-id=\"Data-Preparation-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Data Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Renaming-columns\" data-toc-modified-id=\"Renaming-columns-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Renaming columns</a></span></li><li><span><a href=\"#Dealing-with-missing-data\" data-toc-modified-id=\"Dealing-with-missing-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Dealing with missing data</a></span></li><li><span><a href=\"#Dealing-with-Duplicates\" data-toc-modified-id=\"Dealing-with-Duplicates-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Dealing with Duplicates</a></span></li><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>Feature Engineering</a></span></li><li><span><a href=\"#Data-Spliting\" data-toc-modified-id=\"Data-Spliting-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>Data Spliting</a></span></li><li><span><a href=\"#Droping-Irrelevant-columns\" data-toc-modified-id=\"Droping-Irrelevant-columns-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>Droping Irrelevant columns</a></span></li></ul></li><li><span><a href=\"#Explatory-Data-Analysis\" data-toc-modified-id=\"Explatory-Data-Analysis-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Explatory Data Analysis</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#1.Distribution-of-Ratings\" data-toc-modified-id=\"1.Distribution-of-Ratings-5.0.1\"><span class=\"toc-item-num\">5.0.1&nbsp;&nbsp;</span>1.Distribution of Ratings</a></span></li><li><span><a href=\"#2.Distribution-of-Categories\" data-toc-modified-id=\"2.Distribution-of-Categories-5.0.2\"><span class=\"toc-item-num\">5.0.2&nbsp;&nbsp;</span>2.Distribution of Categories</a></span></li><li><span><a href=\"#3-Distribution-of-Restaurants\" data-toc-modified-id=\"3-Distribution-of-Restaurants-5.0.3\"><span class=\"toc-item-num\">5.0.3&nbsp;&nbsp;</span>3 Distribution of Restaurants</a></span><ul class=\"toc-item\"><li><span><a href=\"#i)cities\" data-toc-modified-id=\"i)cities-5.0.3.1\"><span class=\"toc-item-num\">5.0.3.1&nbsp;&nbsp;</span>i)cities</a></span></li><li><span><a href=\"#ii)-States\" data-toc-modified-id=\"ii)-States-5.0.3.2\"><span class=\"toc-item-num\">5.0.3.2&nbsp;&nbsp;</span>ii) States</a></span></li></ul></li></ul></li><li><span><a href=\"#Popular-Restaurants\" data-toc-modified-id=\"Popular-Restaurants-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Popular Restaurants</a></span></li><li><span><a href=\"#Review-Word-Cloud-Analysis\" data-toc-modified-id=\"Review-Word-Cloud-Analysis-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Review Word Cloud Analysis</a></span></li><li><span><a href=\"#Interactive-Map-Visualization-with-Folium\" data-toc-modified-id=\"Interactive-Map-Visualization-with-Folium-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Interactive Map Visualization with Folium</a></span></li></ul></li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Sentiment-Analysis\" data-toc-modified-id=\"Sentiment-Analysis-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Sentiment Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Feature-Engineering\" data-toc-modified-id=\"Feature-Engineering-6.1.1\"><span class=\"toc-item-num\">6.1.1&nbsp;&nbsp;</span>Feature Engineering</a></span></li></ul></li><li><span><a href=\"#Content-Based-Restaurant-Recommendation\" data-toc-modified-id=\"Content-Based-Restaurant-Recommendation-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Content-Based Restaurant Recommendation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Collaborative-filtering\" data-toc-modified-id=\"Collaborative-filtering-6.2.1\"><span class=\"toc-item-num\">6.2.1&nbsp;&nbsp;</span>Collaborative filtering</a></span></li></ul></li></ul></li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Conclusion</a></span></li><li><span><a href=\"#Recommendations\" data-toc-modified-id=\"Recommendations-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Recommendations</a></span></li><li><span><a href=\"#Future-Improvement-Ideas\" data-toc-modified-id=\"Future-Improvement-Ideas-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Future Improvement Ideas</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afa3010",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "### Introduction\n",
    "\n",
    "In an age where culinary diversity and dining out have become integral parts of our social fabric, choosing the perfect restaurant can be both exciting and overwhelming. With an abundance of dining options ranging from quaint bistros to exotic eateries, making a dining decision has never been more challenging.\n",
    "Traditional restaurant websites have long relied on filters based on amenities, location, or cuisine types, providing users with a plethora of options to sift through. However, as the restaurant industry evolves and culinary landscapes expand, the need for a more refined and personalized approach to restaurant discovery has become evident.\n",
    "Enter the era of restaurant recommendation systemsâ€”a technological marvel that goes beyond the mundane task of filtering restaurants based on their amenities. These systems leverage the power of data science, machine learning, and user preferences to deliver tailored dining suggestions that match your unique tastes and preferences.\n",
    "\n",
    "In a world where time is precious and choices are abundant, restaurant recommendation systems offer an invaluable solution by enhancing the dining experience in ways that traditional filters simply cannot.\n",
    "This project delves into the world of restaurant recommendation systems, exploring their importance, functionality, and the transformative impact they have on the way we discover and enjoy culinary delights.\n",
    "We will unveil how these intelligent algorithms are reshaping the gastronomic landscape, catering to the ever-evolving preferences of diners and revolutionizing the art of restaurant selection. Join us on this journey as we unravel the magic of restaurant recommendation systems, offering a taste of the future of dining exploration.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "This project aims to address the challenge faced by individuals in making informed choices about restaurants and dining experiences by developing a user-friendly restaurant recommendation system that empowers individuals to make informed dining decisions, ultimately enhancing their overall restaurant experience.\n",
    "\n",
    "\n",
    "### Main Objective\n",
    "\n",
    "To develop an interactive and user-friendly restaurant recommendation system.\n",
    "\n",
    "### Specific Objective\n",
    "\n",
    "Analyze key factors for restaurant ratings, identifying and evaluating the key attributes and factors that significantly influence restaurant ratings and customer preferences using data analysis techniques. \n",
    "\n",
    "Develop content-based recommendation algorithms, creating and implementing advanced content-based on algorithms that can generate personalized restaurant recommendations based on user-defined text, restaurant names, and other user preferences. \n",
    "\n",
    "Integrate interactive maps to create an interactive mapping feature within the recommendation system. This map will allow users to explore geographic trends in restaurant recommendations, providing a visually engaging way to discover dining options based on location.\n",
    "\n",
    "Build an interactive user interface that allows users to easily access and interact with the restaurant recommendation system.\n",
    "\n",
    "### Metric of Success\n",
    "\n",
    "To consider our project successful, we will focus on the following key metrics:\n",
    "\n",
    "Our model should effectively address the \"cold start problem,\" ensuring that it can provide meaningful recommendations even for new users or restaurants without extensive review data. \n",
    "Expanding the geographical coverage of our system is another metric. Success is when users from various regions and cities can access relevant restaurant recommendations.\n",
    "The successful deployment of our recommendation model is a critical metric. It should be accessible to users, responsive, and capable of generating real-time recommendations. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5c439a",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "\n",
    "The dataset used in this project, was extracted from the Yelp Restaurant [database](https://www.yelp.com/dataset), which is publicly available and contains a large number of reviews across various restaurants and locations. The dataset contains 908,915 tips/reviews by 1,987,897 users on the  131,930 businesses and their attributes like hours, parking, availability, and ambience aggregated check-ins over time for each. The **dataset contains five jason files namely business.json, checkin.json, review.json, tips.json and user.json**, but only two files were found to containe the relevant required information;\n",
    "> **business.json**: this json file has data on various business all spread over different US states and their relevant attributes.\n",
    "\n",
    "> **review.json**: this json file contains information on reviews made by different users on various business they were served.\n",
    "\n",
    "Due to the dataset being large we have only extracted 54,380 rows and 14 columns which are enough for our analysis and the two above stated json files were merged and only the relevant columns were maintained, named;\n",
    "\n",
    "\n",
    "\n",
    "- **user_id:** A unique identifier for each user who submitted a review\n",
    "\n",
    "- **business_id:** A unique identifier for each business being reviewed\n",
    "\n",
    "- **name:** string, the business's name\n",
    "\n",
    "- **address:** string, the full address of the business \n",
    "\n",
    "- **stars:** The rating given by the user in terms of stars (e.g., 1.0, 2.0, 3.0, 4.0, 5.0),\n",
    "\n",
    "- **text:** The actual text content of the review and\n",
    "\n",
    "- **review_count:** number of reviews the business has received\n",
    "\n",
    "- **city:** string, the city eg \"San Francisco\",\n",
    " \n",
    "- **state:** string, 2 character state code, if applicable eg\"CA\",\n",
    "\n",
    "- **latitude:**  float, latitude of the business\n",
    "\n",
    "- **longitude:** float, longitude of the business\n",
    "\n",
    "- **attributes:** business attributes and features\n",
    "\n",
    "- **categories:** a list of the business categories\n",
    "\n",
    "- **hours:** hours in when the business is open,hours are using a 24hr clock\n",
    "\n",
    "\n",
    "\n",
    "For download of the dataset's, view the [Link](https://www.yelp.com/dataset) anf for complete [documentation](https://www.yelp.com/dataset/documentation/main) of all the datasets.\n",
    "\n",
    "The information contained in this dataset, about business attributes and user reviews, will be used to train models in development of the restaurant reccommendation system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a047da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necesarry packages\n",
    "\n",
    "import collections\n",
    "import folium\n",
    "import json \n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import string\n",
    "import pickle\n",
    "from surprise import Reader , Dataset\n",
    "from tabulate import tabulate\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.prediction_algorithms import SVD\n",
    "from surprise.prediction_algorithms import KNNWithMeans, KNNBasic, KNNBaseline\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# plotting styles\n",
    "plt.style.use(\"fivethirtyeight\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762eb840",
   "metadata": {},
   "source": [
    "> Due to the vast/large nature of our dataset we will only extract a subset of the whole dataset (54,380 entries) from the **business.json** and **review.json**n files for use in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33cca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and extracting data from the business.json file\n",
    "\n",
    "json_data = []\n",
    "lines_to_read = 54380 # Set the number of lines to read\n",
    "\n",
    "# Open the JSON file and parse each line up to the specified limit\n",
    "with open('./data/yelp_academic_dataset_business.json', 'r') as file:\n",
    "    for i, line in enumerate(file):  # looping over the file entries\n",
    "        if i >= lines_to_read:       # stopping when the number of required lines are met\n",
    "            break\n",
    "        json_object = json.loads(line) \n",
    "        json_data.append(json_object)\n",
    "\n",
    "# converting the json_data list into a pandas dataframe\n",
    "business = pd.DataFrame(json_data)\n",
    "\n",
    "# saving the file for future easier retreival\n",
    "business.to_csv(filepath='./data/business.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8881b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading and extracting data from the review.json file\n",
    "\n",
    "json_data = []\n",
    "lines_to_read = 54380  # Set the number of lines to read\n",
    "\n",
    "# Open the JSON file and parse each line up to the specified limit\n",
    "with open('./data/yelp_academic_dataset_review.json', 'r') as file:\n",
    "    for i, line in enumerate(file):  # looping over the file entries\n",
    "        if i >= lines_to_read:       # stopping when the number of required lines are met\n",
    "            break\n",
    "        json_object = json.loads(line)\n",
    "        json_data.append(json_object)\n",
    "\n",
    "# converting the json_data list into a pandas dataframe\n",
    "review = pd.DataFrame(json_data)\n",
    "\n",
    "# saving the file for future easier retreival\n",
    "review.to_csv(filepath='./data/review.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a8a77",
   "metadata": {},
   "source": [
    "> Loading the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbbea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the businesses dataset\n",
    "business= pd.read_csv(\"./data/business.csv\")\n",
    "# previewing the datasetdata.loc[ data.categories.str.contains('Restaurants')]\n",
    "business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the datasets features/columns\n",
    "business.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d56e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the second dataset review.csv\n",
    "review=pd.read_csv(\"./data/review.csv\")\n",
    "# previewing the dataset\n",
    "review.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8124b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# viewing the datasets features/columns\n",
    "review.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e90aacb",
   "metadata": {},
   "source": [
    "> After previewing both of the datasets, we now merge them into one dataset, using the **business_id** column as primary key so as to obtain all the feature in one dataset for easier analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5fa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging the two datasets into one using the business_id primary key\n",
    "\n",
    "data=pd.merge(left=review , right=business, how='left', on='business_id')\n",
    "\n",
    "# previewing the new merge dataset\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3319d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previewing the dataset \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cdb568",
   "metadata": {},
   "source": [
    "> After merging both the datasets we have obatined a dataframe with 54,380 rows and 21 columns, we then proceed to data preparation for more processing of the dataframe in preparation for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b30c7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6f007",
   "metadata": {},
   "source": [
    "From the above output we can conclude that;\n",
    "- The avarge rating given by users to different businesses is 3.8 $ \\approx$ 4 , with 50% of the ratings being between 3 and 5 rating\n",
    "- The avarge star rating for the various businesses is 3.7 $ \\approx$ 4 , with majority of the businesses having 3.5 and 4 star ratings.\n",
    "- The avarage number of reviews in our data made by the users regarding different business is 389, with majority of the businesses having reviews between 61 and 430. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39818ed",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "In this section, we will perform data cleaning to prepare the dataset for analysis, the various data cleaning methods that are to be used will be;\n",
    "\n",
    "- Renaming columns\n",
    "- Checking Dealing with missing data\n",
    "- Checking and removing duplicates \n",
    "- Feature Engineering\n",
    "- Selecting the Relevant Columns\n",
    "- Droping Irrelevant columns\n",
    "- Selecting relevant rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341deb76",
   "metadata": {},
   "source": [
    "### Renaming columns\n",
    "\n",
    "Renaming the **stars_x** and **stars_y** columns into **rating** and **b/s_rating** columns for better understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fc9725",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'stars_x':'rating', 'stars_y':'b/s_rating'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b92039f",
   "metadata": {},
   "source": [
    "### Dealing with missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ca730b",
   "metadata": {},
   "source": [
    "Missing values can lead to gaps in your dataset, making it incomplete and potentially unreliable for analysis. By addressing missing values, you ensure that you have a more comprehensive dataset to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db7e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for columns with missing values\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a25097d",
   "metadata": {},
   "source": [
    "> The main columns in our analysis, that will be used to develope the recommendation system are **user_id**, **business_id** and **rating**. Therefore, since the columns that have missing values only provide metadata/ more information about our restaurants i.e **address, attribute, categories** **and hours**, then we will impute the missing values with \"Not-Avaliable\" since they won't influence our analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2824681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values in the address column with \"Not-Available\"\n",
    "data.address.fillna(value=\"Not-Available\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e088a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values in the attributes column with \"Not-Available\" \n",
    "data.attributes.fillna(value=\"Not-Available\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27536c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values in the categories column with \"Not-Available\" \n",
    "data.categories.fillna(value=\"Not-Available\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6e28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values in the hours column with \"Not-Available\" \n",
    "data.hours.fillna(value=\"Not-Available\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80613401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# previewing our changes\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda34f27",
   "metadata": {},
   "source": [
    "### Dealing with Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5c5c4f",
   "metadata": {},
   "source": [
    "In order to avoid inaccurate insights we have to deal with duplicate data becuase deuplicate data can lead to incorrect or misleading insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85bf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicated columns\n",
    "print(\"Duplicates: \",data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f715906",
   "metadata": {},
   "source": [
    "> - Our data has no duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1dae98",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "In feaure engineering, we will combine the **city**, **state**, **address** columns to form one column, **location** column with the intention to perform geospatial analysis or visualization. Having a unified location column simplifies the process and you can use the location column directly to create maps or explore geographic trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0c7c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining the address columns\n",
    "data['location']=data[['city','state','address']]\\\n",
    "            .apply( lambda x: f\"State:{x['state']}, City:{x['city']}, Address:{x['address']} \", axis=1)\n",
    "\n",
    "# then we drop the combined columns\n",
    "data.drop(columns=['state', 'city','address'], axis=1, inplace=True)\n",
    "\n",
    "data.location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23197a0",
   "metadata": {},
   "source": [
    "> Then we will convert the **user_id** column form string into intergers, by assigning the uniques string ids interger values. This will aid in our modeling process in the later sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae49923c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the user_id into intergers\n",
    "\n",
    "# selecting only the uniques user ids as a dataframe\n",
    "ids=data[['user_id']].drop_duplicates('user_id').reset_index(drop=True).copy()\n",
    "\n",
    "# resetting the indexes, to include a continous numbering \n",
    "ids=ids.reset_index()\n",
    "\n",
    "# merging the ids dataframe with our original dataframe using the user id column as primary key\n",
    "# renaming the index column to represent the user ids\n",
    "data=pd.merge(data,ids, how='left', on='user_id').drop('user_id', axis=1).rename(columns={'index':'user_id'})\n",
    "\n",
    "# writting a function to order the user ids to start from 1 instead of '0'\n",
    "def add(x):\n",
    "    \"\"\" adds 1 to the existing user id\"\"\"\n",
    "    y=x+1\n",
    "    return y\n",
    "data.user_id=data.user_id.apply(add )  # applyng the function to our user ids\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935382c4",
   "metadata": {},
   "source": [
    "> Creating a new column called the **price** column from the information contained in  the **attributes column**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09f3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function to extract the price values\n",
    "\n",
    "def Price(val):\n",
    "    \"\"\"\n",
    "    The function takes in a dictionary and extracts the price in the 'RestaurantsPriceRange2' key, else returnsa '0'\n",
    "    if the value if 'Not-Available'\n",
    "    \"\"\"\n",
    "    # evaluates the attributes values as dictionary because they have the dictionary structure but are of type string\n",
    "    try:\n",
    "        p = eval(val)['RestaurantsPriceRange2']    # extracts the price values in the pricerange  key\n",
    "        return int(p)                              # returns the price as interger\n",
    "    except:\n",
    "        return 0                                   # else returns zero if the pricerange key is not listed\n",
    "    \n",
    "# applying the function to the attributes column\n",
    "data['price']=data.attributes.apply(Price)\n",
    "\n",
    "# previewing the column\n",
    "data[['price']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53249e67",
   "metadata": {},
   "source": [
    "### Data Spliting\n",
    "\n",
    "Since our data contains several businesses other than restaurants eg Gyms, Saloons, Hardwares.. , we will only select the businesses that are restaurants. Working with a single business category simplifies the problem complexity and sets us in course with our objective and is essential to obtain relevant findings and recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b1ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only the restaurants\n",
    "data=data.loc[ data.categories.str.contains('Restaurants')].copy().reset_index(drop=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ee3da",
   "metadata": {},
   "source": [
    "### Droping Irrelevant columns\n",
    "The columns **review_id , useful , funny , cool , is_open , postal_code and date** are not informative in our analysis and will not be used during analysis, therefore we drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffa0b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# droping unrelevant collumn\n",
    "cols=['review_id', 'useful','postal_code','funny', 'cool', 'is_open', 'date']\n",
    "data.drop(columns=cols, axis=1, inplace=True)\n",
    "\n",
    "# confirming the remaining columns\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38f23b4",
   "metadata": {},
   "source": [
    "## Explatory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03292ddd",
   "metadata": {},
   "source": [
    "#### 1.Distribution of Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1de5db",
   "metadata": {},
   "source": [
    "1) What are the differences in the distribution of user ratings and business ratings in the dataset and what do these differences indicate about user preferences?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e202be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have a DataFrame named 'df' and two numerical columns 'column1' and 'column2'\n",
    "column1 = 'rating'\n",
    "column2 = 'b/s_rating'\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))  # Adjust the figsize as needed\n",
    "\n",
    "# Create histograms for column1 and column2\n",
    " \n",
    "sns.countplot(data=data, x=column1 ,ax=axes[0] , color='tab:blue')\n",
    "# Adjust the number of bins as needed\n",
    "axes[0].set_xlabel(\"User ratings\")\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'Histogram of User ratings')\n",
    "\n",
    "sns.countplot(data=data, x=column2 ,ax=axes[1] ,color='tab:blue')\n",
    "  # Adjust the number of bins as needed\n",
    "axes[1].set_xlabel(\"Business ratings\")\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title(f'Histogram of Business ratings')\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d456f332",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = data[['rating', 'b/s_rating', 'review_count']]\n",
    "df_1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a400737",
   "metadata": {},
   "source": [
    "**Correlation between Rating and B/S Rating**\n",
    "\n",
    "Correlation Value: 0.409173 The positive correlation coefficient of approximately 0.41 suggests a moderate positive relationship between the rating of a restaurant or service and its B/S Rating. In other words, as the rating of a business increases, its B/S Rating tends to increase as well. Businesses with higher individual ratings are likely to have a higher overall rating based on user reviews.\n",
    "\n",
    "**Correlation between Rating and Review Count**\n",
    "\n",
    "Correlation Value: 0.106111 The correlation coefficient of about 0.11 indicates a weak positive correlation between the rating of a restaurant or service and its review count.This suggests that businesses with higher ratings may have slightly more reviews, but the relationship is not particularly strong.\n",
    "\n",
    "**Correlation between B/S Rating and Review Count**\n",
    "\n",
    "Correlation Value: 0.237139 The correlation coefficient of approximately 0.24 suggests a moderate positive correlation between the B/S Rating of a business and its review count. This implies that businesses with higher B/S Ratings tend to have more user reviews. Customers are more likely to review businesses with higher overall ratings, contributing to the positive correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f30716",
   "metadata": {},
   "source": [
    "#### 2.Distribution of Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cebc272e",
   "metadata": {},
   "source": [
    "1) What are the most prevalent restaurant categories and how does the distribution of these categories impact the restaurant landscape in terms of user preferences and choices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a357b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# list of all categories\n",
    "categories=[ cat for category in data.drop_duplicates('business_id').categories for cat in category.split(',')]\n",
    "# list of unique categories only\n",
    "categories=collections.Counter(categories)\n",
    "#picking the top 20 categories\n",
    "common=categories.most_common(12)\n",
    "# ploting\n",
    "fig, ax=plt.subplots(figsize=(8,5))\n",
    "x=[ i[0] for i in common[2:]]\n",
    "y=[i[1] for i in common[2:]]\n",
    "sns.barplot(x=x, y=y, color='tab:blue', ax=ax)\n",
    "ax.set_xlabel(\"Categories\")\n",
    "ax.tick_params(axis='x', labelrotation=90)\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.set_ylim([300,900])\n",
    "ax.set_title(\"Top 10 Restaurant Categories\");\n",
    "ax.bar_label( ax.containers[0], padding=3, fmt='{:,}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dec5a7",
   "metadata": {},
   "source": [
    "The plot of restaurant categories reveals that the dataset predominantly consists of food-related establishments, signifying that restaurants serving a variety of cuisines hold the highest representation. Following closely are nightlife-related venues, suggesting a vibrant nightlife scene in the dataset. Conversely, fast food and burger establishments are among the less frequently occurring categories, indicating their lower prevalence within the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70ed73a",
   "metadata": {},
   "source": [
    "#### 3 Distribution of Restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ad22c9",
   "metadata": {},
   "source": [
    "##### i)cities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322395c4",
   "metadata": {},
   "source": [
    "1) What are the most frequently occurring cities and how does the distribution of restaurants in these cities impact users' choices of restaurants?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730506ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract the location column and drop duplicate business entries\n",
    "location= data.drop_duplicates('business_id')[['location']]\n",
    "# split the location string to extract the city part, removing city from each entry\n",
    "city=location.location.apply(lambda x: x.split(',')[1].replace(\"City:\",'')) \n",
    "# count the occurrences of each city and selected the top 10 cities\n",
    "city=collections.Counter(city)\n",
    "city=city.most_common(10)\n",
    "# extract the city names and their respective counts\n",
    "y=[i[0] for i in city]\n",
    "x=[i[1] for i in city]\n",
    "# created a barplot to visualize the city counts\n",
    "fig, ax=plt.subplots(figsize=(7,5))\n",
    "sns.barplot( y=y, x=x, color='tab:blue')\n",
    "ax.set_ylabel(\"City Names\")\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"City Counts\");\n",
    "ax.bar_label( ax.containers[0],padding=3, fmt='{:,}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7234a8bb",
   "metadata": {},
   "source": [
    "The analysis of restaurant locations reveals that Philadelphia has the highest number of restaurants in the dataset, making it the most prevalent city. Following, but not as closely, is Tampa, indicating a notable restaurant presence. In contrast, the cities of Reno and Santa Barbara have fewer restaurants, making them less common in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd9256",
   "metadata": {},
   "source": [
    "##### ii) States"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063e08d",
   "metadata": {},
   "source": [
    "2) What are the most common states and how does the distribution of restaurants across these states impact the types of cuisine available and user preferences for dining?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df0ff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the location column and split it to obtain the state part\n",
    "states=location.location.apply(lambda x: x.split(',')[0].replace(\"State:\",'')) \n",
    "# count the occurrences of each state and select the top 10 states\n",
    "states=collections.Counter(states)\n",
    "states=states.most_common(10)\n",
    "# extract the state names and their respective counts for visualization\n",
    "y=[i[0] for i in states]\n",
    "x=[i[1] for i in states]\n",
    "# created a barplot to visualize the state counts\n",
    "fig, ax=plt.subplots(figsize=(8,4))\n",
    "sns.barplot( y=x,x=y , color='tab:blue')\n",
    "ax.set_ylabel(\"States\")\n",
    "ax.set_xlabel(\"Count\")\n",
    "ax.set_title(\"State Counts\");\n",
    "ax.bar_label( ax.containers[0],padding=3, fmt='{:,}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb590d15",
   "metadata": {},
   "source": [
    "The analysis of restaurant locations by state reveals that Philadelphia has the highest number of restaurants in the dataset, making it the most prevalent state. Following, but not as closely, is Florida, indicating a notable presence of restaurants in that state. In contrast, Alberta and Nevada have fewer restaurants, making them less common in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b49953",
   "metadata": {},
   "source": [
    "### Popular Restaurants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49e2b81",
   "metadata": {},
   "source": [
    "Analyzing and highlighting the most popular restaurants is essential because it allows us to recognize the establishments that have garnered significant attention and interest from customers. This information will be valuable for consumers seeking highly-rated dining experiences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# most popular restaurants\n",
    "\n",
    "index=data.drop_duplicates(subset='business_id').sort_values(by=['review_count','b/s_rating'],ascending=False)[:10].index\n",
    "\n",
    "fig, ax=plt.subplots(figsize=(7,5))\n",
    "sns.barplot(data=data.loc[index], x=\"review_count\", y='name', color='tab:blue')\n",
    "ax.set_ylabel(\"Resturant Names\")\n",
    "ax.set_xlabel(\"Review Counts\")\n",
    "ax.set_xlim([1500,4600])\n",
    "ax.set_title(\"Most Popular Restaurants\");\n",
    "ax.bar_label( ax.containers[0],padding=3, fmt='{:,}');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d46176a",
   "metadata": {},
   "source": [
    "The output highlights the review counts for the different restaurants in descending order. At the top is Luke with an impressive 4554 reviews, signifying that it has received a substantial amount of customer feedback. Following closely is the Santa Barbara Shellfish Company with a notable 2404 reviews, demonstrating a strong presence and popularity among diners. Cafe Fleur De Lis is the second last on the list, with a still respectable 1865 reviews, though a bit lower than the previous two. Finally, Milk and Honey Nashville rounds out the list with 1725 reviews, indicating a significant number of satisfied patrons. These review counts offer insights into the popularity and customer engagement of these respective restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1085883",
   "metadata": {},
   "source": [
    "For analyzing the relationship between business rating and restaurant price, we will be creating a box plot. A box plot can provide more detailed insights into the distribution of price ranges for different rating categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e85f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing: Drop duplicate businesses and extract prices\n",
    "df = data.drop_duplicates('business_id')\n",
    "\n",
    "# Count and print the number of businesses without price information\n",
    "print(\"Number of businesses without price: \", sum([1 for i in df.price.values if i == 0]))\n",
    "\n",
    "# Create a box plot\n",
    "fig, ax = plt.subplots(figsize=(16, 6))\n",
    "sns.boxplot(x='b/s_rating', y='price', data=df, ax=ax)\n",
    "ax.set_xlabel(\"Business Rating\")\n",
    "ax.set_ylabel(\"Price\")\n",
    "ax.set_yticks([0, 1, 2, 3, 4, 5])\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "ax.set_title(\"Business Rating vs. Price Range\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4eeff6",
   "metadata": {},
   "source": [
    "The number of businesses without price is 127. This indicates that out of the analyzed businesses, 127 have missing or unspecified price information. It's important to note that the central line in each box represents the median price, which is a robust measure of the central tendency of restaurant prices for different rating levels. The vertical span of each box shows the interquartile range, providing insights into the spread or variability of prices within each rating category. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4201dc0",
   "metadata": {},
   "source": [
    "To visualize the relationship between business rating and the number of reviews more effectively, we will be creating a 2D histogram or a hexbin plot. These plots provide a better representation of the density and concentration of data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0fe24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16,5))\n",
    "\n",
    "# Create a hexbin plot\n",
    "hb = ax.hexbin(data['review_count'], data['b/s_rating'], gridsize=50, cmap='viridis')\n",
    "\n",
    "ax.set_xlabel(\"Number of Reviews\")\n",
    "ax.set_ylabel(\"Rating\")\n",
    "ax.set_title('Business Rating Against Number of Reviews (Hexbin Plot)')\n",
    "\n",
    "# Add a colorbar to indicate the density of points\n",
    "cb = plt.colorbar(hb)\n",
    "cb.set_label('Density')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab304243",
   "metadata": {},
   "source": [
    "### Review Word Cloud Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0384da",
   "metadata": {},
   "source": [
    "To create a word cloud visualization that highlights the most frequent words found in review texts column. By filtering and analyzing the text of positive reviews, the word cloud offers an intuitive and visually appealing way to identify the key themes and frequently mentioned aspects that contribute to a positive customer experience. This visualization provides valuable insights into the factors that most positively influence customer satisfaction, aiding in the understanding of what makes certain businesses successful and well-reviewed on Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37c9bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word count plot for the positive reviews\n",
    "\n",
    "# selecting only positive reviews is reveiws with 4 and above ratings\n",
    "df=data.loc[ (data['rating']>=4)& (data['b/s_rating']>=4)] \n",
    "\n",
    "# clearing the plot figure\n",
    "plt.figure(figsize=(15,15))            \n",
    "\n",
    "plt.title('The Most Common Word in Positive Review Texts\\n', fontsize=20)  # setting plot title\n",
    "\n",
    "# creating the plot on only the most 200 words and setting the plot size\n",
    "wc = WordCloud(max_words=200, min_font_size=10,height=500,width=1000,background_color=\"white\")\\\n",
    "        .generate(' '.join(df['text'])) #create a WordCloud uding the text review column\n",
    "\n",
    "plt.imshow(wc);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4ac3ab",
   "metadata": {},
   "source": [
    "The word cloud visually represents the most common words found in the positive review texts, where the size of each word in the cloud corresponds to its frequency in the reviews. This provides an overview of the key terms and themes in positive reviews.\n",
    "> Therefore from the plot above, the most occuring words in the positive reviews are words like , **good, food, place , great , delicious , great , service , amazing , best,..etc**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8f035f",
   "metadata": {},
   "source": [
    "To create a word cloud visualization that highlights the most frequently occurring words in negative review texts from the Yelp dataset. By filtering and analyzing the text of negative reviews, the word cloud offers a visual representation of the common themes and frequently mentioned issues that contribute to a negative customer experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b742e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a word count plot for the positive reviews\n",
    "\n",
    "# selecting only negative reviews i.e reveiws with 2 and below ratings\n",
    "df=data.loc[ (data['rating']<=2)& (data['b/s_rating']<=2)]\n",
    "plt.figure(figsize=(15,15))\n",
    "\n",
    "# clearing the plot figure\n",
    "plt.title('The Most Common Word in Negative Review Texts\\n', fontsize=20)\n",
    "\n",
    "\n",
    "# creating the plot on only the most 200 words and setting the plot size\n",
    "wc = WordCloud(max_words=200,height=500,width=1000,background_color=\"white\")\\\n",
    "        .generate(' '.join(df['text']))    #create a WordCloud uding the text review column\n",
    "\n",
    "plt.imshow(wc);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6d530",
   "metadata": {},
   "source": [
    "This visualization provides valuable insights into the factors that may lead to poor reviews, helping to identify areas of improvement for businesses and assisting in understanding the aspects that negatively impact customer satisfaction on Yelp.\n",
    "> Therefore from the plot above, the most occuring words in the negative reviews are words like , **minute , food, place , ordered, pizza, terrible, time, service, said ..etc**, these words are attributed to disatisfied customers and  issuence of bad /poor ratings to restautrants. When such words are encounterd there shoud be further investigation of the business service delivary for improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ab7c2",
   "metadata": {},
   "source": [
    "### Interactive Map Visualization with Folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b306c1",
   "metadata": {},
   "source": [
    "The idea is to write a code that generates an interactive map using Folium, a Python library for creating leaflet maps. The map is centered at a specific latitude and longitude, which is determined by the first entry in the dataset. It then proceeds to populate the map with markers for selected businesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2537670a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set up center latitude and longitude\n",
    "center_lat = data['latitude'][0]\n",
    "center_long = data['longitude'][0]\n",
    "\n",
    "# Initialize map with center lat and long\n",
    "map_ =folium.Map([center_lat,center_long], zoom_start=5)\n",
    "\n",
    "# Adjust this limit to see more or fewer businesses\n",
    "limit=1000\n",
    "\n",
    "for index in range(limit):\n",
    "    # Extract information about business\n",
    "    lat = data.loc[index,'latitude']\n",
    "    long = data.loc[index,'longitude']\n",
    "    name = data.loc[index,'name']\n",
    "    rating = data.loc[index,'b/s_rating']\n",
    "    location = data.loc[index,'location']\n",
    "    details = \"{}\\nStars: {} {}\".format(name,rating,location)\n",
    "    \n",
    "    # Create popup with relevant details\n",
    "    popup = folium.Popup(details,parse_html=True)\n",
    "    \n",
    "    # Create marker with relevant lat/long and popup\n",
    "    marker = folium.Marker(location=[lat,long], popup=popup)\n",
    "    \n",
    "    marker.add_to(map_)\n",
    "    \n",
    "map_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bdc065",
   "metadata": {},
   "source": [
    "The result is an interactive map with markers representing businesses. When you click on a marker, a popup appears with details about the business, such as its name, rating, and location. This visualization provides a spatial perspective on the distribution of businesses in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361e54e",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "Now the next step is to perform some text preprocessing steps which include:\n",
    "\n",
    "- Feature engineering : futher feature engineering of the columns to meet the required specifications for analysis eg aggregating text reviews, creating new columns form the existing columns .. etc\n",
    "- Removal of Punctuations and Removal of Stopwords - we'll make use of the **RegexpTokenizer()** method\n",
    "-  Stemming - reducing words to their oot meaning , we'll use the **SnowballStemmer()** method\n",
    "- Word-Vectorization - splitting text data into a vector of individual words for further and easier nlp analysis, we'll make use of the **TidfVectorizer()** method that vectorizes text data and calculates their respective Term Frequency - Inverse Document Frequency (TI-IDF) values.\n",
    "\n",
    "Now letâ€™s perform the above text preprocessing steps on the data:\n",
    "\n",
    "#### Feature Engineering \n",
    " \n",
    "This feature engineering step helps prepare your data for analysis and modeling by selecting and transforming the most relevant attributes, which can lead to more effective modeling and improved insights for our project.\n",
    "> We'll start by creating a new **review column** that aggregates all the text reviews partaining a single restaurant from all the users into one text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7087b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a function that aggregates/combines all the extra reviews made to a particular restaurant into one text\n",
    "\n",
    "def new_df(data):\n",
    "    \"\"\"\n",
    "    The function takes in a dataframes and groups it by the business)id column then combines all the text values in the\n",
    "    text column into one big text then assigns it to the review column\n",
    "    \n",
    "    \"\"\"\n",
    "    # drop duplicates based on business_id and reset the index\n",
    "    df = data.drop_duplicates('business_id').reset_index(drop=True)\n",
    "    \n",
    "    # loop through unique business_id values\n",
    "    for id in data.business_id.unique():\n",
    "    # extract text for each unique business_id and explode it into separate rows\n",
    "        text = data.loc[data.business_id == id, 'text'].explode(ignore_index=True)\n",
    "    # join the exploded text into a single string\n",
    "        text = ' '.join(text)\n",
    "        \n",
    "    # assign the concatenated text to the reviews column for the corresponding business_id\n",
    "        df.loc[data.business_id == id, 'reviews'] = text\n",
    "    \n",
    "    return df\n",
    "\n",
    "# call the function and create the new df\n",
    "df = new_df(data)\n",
    "df.head()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eab91b",
   "metadata": {},
   "source": [
    "> We'll the create a new column called **attributes_true** by only extracting the true business attributes contained in the attributes column that has dictionary like entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5b611d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decompressing the attributes column into  new 'attributes_true' column\n",
    "\n",
    "def decompress(x):\n",
    "    \"\"\"\n",
    "    The function takes in a dictionary and returns only the keys that have their values not being False   \n",
    "    \"\"\"\n",
    "      \n",
    "    list_ = []\n",
    "# evaluate the attributes column to convert it from a string to a dictionary\n",
    "    data_dict = eval(x)\n",
    "    \n",
    "# iterate through the key-value pairs in the dictionary\n",
    "    for key, val in data_dict.items():\n",
    "# check if the key is in the specified categories and if the value is not \"None\"\n",
    "        if (key in ['Ambience', 'GoodForMeal', 'BusinessParking']) and (val != \"None\"):\n",
    "# if conditions are met, further iterate through sub-dictionary\n",
    "            for key_, val_ in eval(data_dict[key]).items():\n",
    "# if the sub-dictionary value is true, append it to the list\n",
    "                if val_:\n",
    "                    list_.append(f'{key}_{key_}')\n",
    "        else:\n",
    "# if the value is not false, append the key to the list\n",
    "            if val != 'False':\n",
    "                list_.append(key)\n",
    "    \n",
    "# join the list of selected attribute names into a space-separated string\n",
    "    return \" \".join(list_)\n",
    "\n",
    "# create a new column 'attributes_true' in the df by applying the decompress function\n",
    "# include a condition to handle cases where attributes is 'Not-Available'\n",
    "df['attributes_true'] = df.attributes.apply(lambda x: decompress(x) if x != 'Not-Available' else ' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a48f9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming if the new created column has performed as expected\n",
    "\n",
    "print(\"Before:\")\n",
    "print(eval(df.attributes[0]))\n",
    "print('\\n After:')\n",
    "df['attributes_true'][0]      # Print the result for the first row of 'attributes'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26f08af",
   "metadata": {},
   "source": [
    ">From the above output we can see that the function has only retrieved keys that have their values not egual to 'False'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0347275",
   "metadata": {},
   "source": [
    "> - We will then merge the **attributes_true, categories, reviews** columns into one large text for each unique business and assign to a new column **details**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a268cf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging different columns to form one column of text \n",
    "df['details']=df[['attributes_true','categories','reviews']].apply(lambda x: ''.join(x), axis=1)\n",
    "\n",
    "# previewing the first row value in the new column\n",
    "df.details[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1220bf25",
   "metadata": {},
   "source": [
    "> After creating our desired column **details** , w'll then drop the columns that will not be usefull onwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2ba9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns\n",
    "df.drop(columns=['attributes_true','reviews'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e73096",
   "metadata": {},
   "source": [
    "From the text example above we can see that the column text contains many symbols, punctuations and stop word, next we shall remove the symbols and tokenize the column into a bag of words. These reasons serve to prepare text data for various text analysis and NLP tasks. It tokenizes the text, applies stemming, and standardizes the text for downstream processing, making it easier to analyze and extract meaningful information from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b57fcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# firts create a pattern that strips all the no word characters from words during tokenization\n",
    "pattern =r\"(?u)\\b\\w\\w+\\b\"\n",
    "\n",
    "# instantiate the tokenizer\n",
    "tokenizer = RegexpTokenizer(pattern)\n",
    "\n",
    "# instantiating the stemmer\n",
    "stemmer = SnowballStemmer(language=\"english\")\n",
    "\n",
    "# creating a function to tokenize and stem words\n",
    "def stem_and_tokenize(list_):\n",
    "    tokens = tokenizer.tokenize(list_)\n",
    "    return [stemmer.stem(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca01ae63",
   "metadata": {},
   "source": [
    "After instantiating the tokenizer and stemmer we then calculate the text frequency-inverse document frequency values using the  **TfidfVectorizer()** method. Calculating TF-IDF values is a crucial step in preparing text data for analysis and transforming it into a format suitable for many NLP and text mining tasks. It helps convert unstructured text into structured numerical data that can be used for various analytical and machine learning purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb1e101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiating the stop words\n",
    "stopwords=stopwords.words('english')\n",
    "# stemming the stopwords for uniformity during removing stopwords\n",
    "stopwords=[ stemmer.stem(i) for i in stopwords]\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer( max_features=200 , \n",
    "                        stop_words=stopwords,\n",
    "                        tokenizer= stem_and_tokenize\n",
    "#                         ngram_range=(1, 2), \n",
    "#                         min_df=0, \n",
    "                        )\n",
    "\n",
    "\n",
    "# fitting and transforming the details column to extract the top 200 features\n",
    "tfidf_matrix=tfidf.fit_transform(df['details'])\n",
    "\n",
    "# previewing the tfidf matrix\n",
    "pd.DataFrame.sparse.from_spmatrix(tfidf_matrix, columns=tfidf.get_feature_names_out()).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541462a0",
   "metadata": {},
   "source": [
    "The code is configuring and using a TF-IDF vectorizer to convert text data into a numerical representation that captures word importance, while stemming stopwords for uniformity. The warning message is related to an unused parameter and does not affect the execution of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6df8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a matrix of the cosine similarities of the various rows based on the tidf scores\n",
    "cosine_similarity=cosine_similarity(tfidf_matrix)\n",
    "print(\"shape: \",cosine_similarity.shape)\n",
    "\n",
    "# viewing the first column\n",
    "cosine_similarity[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8901c45",
   "metadata": {},
   "source": [
    "The code is calculating the cosine similarity between the rows of the TF-IDF matrix (tfidf_matrix). The cosine similarity is a measure of similarity between two non-zero vectors in an inner product space, often used for text document similarity calculations. In this case, it's used to measure the similarity between the 'details' text descriptions of different businesses based on their TF-IDF scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c92e3cc",
   "metadata": {},
   "source": [
    "> We will the pickle our desired data for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a30aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pickle.dump(tfidf_matrix, open('./data/tfidf_matrix.pkl', 'wb'))\n",
    "pickle.dump(cosine_similarity, open('./data/cosine_similarity.pkl', 'wb'))\n",
    "pickle.dump(df, open('./data/data.pkl', 'wb'))\n",
    "print(\"Files saved...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3501653",
   "metadata": {},
   "source": [
    "### Content-Based Restaurant Recommendation \n",
    "\n",
    "Using the cosine similarity matrix we will now create a content-based recommendation system that offers recommendations to users based on the restaurant names or text words representing the specifications of their desired restaurant and attributes.\n",
    "\n",
    "\n",
    "> We use the cosine similarity matrix to compare similarity between different restaurants and the customers prefrences, then pick the top n similar restaurants to recommend based on his/her input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3810552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a folium_map function that displays restaurant lovations\n",
    "\n",
    "def folium_map(data):\n",
    "    \"\"\"\n",
    "    The function takes in a dataframe and using the latitude and longitude columns displays a map showing the locations of \n",
    "    all the restaurants avaible in the input data\n",
    "    \"\"\"\n",
    "    # reseting the index in the input dataframe\n",
    "    dff=data.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Set up center latitude and longitude\n",
    "    center_lat = dff['latitude'][0]\n",
    "    center_long = dff['longitude'][0]\n",
    "\n",
    "# Initialize map with center lat and long\n",
    "    map_ =folium.Map([center_lat,center_long], zoom_start=7)\n",
    "\n",
    "# Adjust this limit to see more or fewer businesses\n",
    "    limit=dff.shape[0]\n",
    "    print(f\"{limit} Resturant Locations\")\n",
    "    for index in range(limit):\n",
    "        # Extract information about business\n",
    "        lat = dff.loc[index,'latitude']\n",
    "        long = dff.loc[index,'longitude']\n",
    "        name = dff.loc[index,'name']\n",
    "        rating = dff.loc[index,'b/s_rating']\n",
    "        location = dff.loc[index,'location']\n",
    "        details = \"{}\\nStars: {} {}\".format(name,rating,location)\n",
    "\n",
    "# Create popup with relevant details\n",
    "        popup = folium.Popup(details,parse_html=True)\n",
    "\n",
    "# Create marker with relevant lat/long and popup\n",
    "        marker = folium.Marker(location=[lat,long], popup=popup)\n",
    "\n",
    "        marker.add_to(map_)\n",
    "\n",
    "    return display(map_)  # returning a map display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e5531",
   "metadata": {},
   "outputs": [],
   "source": [
    "folium_map(data=df.loc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d770356",
   "metadata": {},
   "source": [
    "The content_based function uses content-based recommendation techniques to provide restaurant recommendations based on user input preferences, restaurant names, or user-defined text. The recommendations can be filtered by minimum rating and location and are visually presented on an interactive map if specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403b01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_based(df=df, name:str= None , rating:int =1, num:int=5, text: str=None, location:str = None):\n",
    "    \"\"\"\n",
    "    The function takes the following input;\n",
    "    \n",
    "    df: DataFrame - a dataframe containing unique rseturants\n",
    "    name: str - name of restaurant to recomend similar restaurants\n",
    "    num:int - number of restaurants to recommend\n",
    "    location: string - prefered location\n",
    "    rating: string - prefered rating of restaurant\n",
    "    text: - User prefrencese inform of text\n",
    "    \n",
    "    Then based on the input parameters offers similar retaurants according to the input parameters to users\n",
    "    \"\"\"\n",
    "    \n",
    "    if name:\n",
    "        index_=df.loc[df.name== name].index[0]                          # find the index of the input name\n",
    "        sim=list(enumerate(cosine_similarity[index_]))                  # extract similarity vector of that name index\n",
    "        sim=sorted(sim, key=lambda x: x[1], reverse=True)[1:num+1]      # arrange the vector values in ascending order\n",
    "        indices= [i[0] for i in sim]                                    # Extract the indices of the top high scores\n",
    "        print(f\"Top {num} Restaurants Like [{name}]\")\n",
    "        \n",
    "        # if the location parameter is passed then the dataframe is filtered based on the input location\n",
    "        if location:                                                \n",
    "            df=df.loc[ (df['b/s_rating']>=rating) & ( df.location.str.contains(location))]\n",
    "            folium_map(df)\n",
    "        else: \n",
    "            df= df.loc[ (df['b/s_rating']>=rating) ] \n",
    "        # filtering the data based on the selected indices    \n",
    "        df=df.loc[indices,('name','b/s_rating','review_count','location')].sort_values('b/s_rating', ascending=False)\n",
    "        return  df.reset_index(drop=True)\n",
    "    \n",
    "    # if the name is None then switch to other parameters\n",
    "    else:\n",
    "        # if the text has a passed input values then this if statement runs            \n",
    "        if text: \n",
    "                text=text.lower()                                           # converting the text into lowercase\n",
    "                tokens=stem_and_tokenize(text)                              # tokenizing and stemming the words\n",
    "                tokens=[ word for word in tokens if word not in stopwords]  # removing stopwords\n",
    "                text_set=set(tokens)                                        # taking only unique words\n",
    "                \n",
    "                if location: # using entered location to filter the data\n",
    "                    df=df.loc[ (df.location.str.contains(location)) & (df['b/s_rating']>=rating)].reset_index(drop=True)\n",
    "\n",
    "                vectors=[] # creating an emplty list to append the intersection values\n",
    "                for words in df.details:                                     # looping over the text in the details column\n",
    "                    words=words.lower()                                      # lowering the text\n",
    "                    words=stem_and_tokenize(text)                            # tokenizing and stemming the words\n",
    "                    words=[ word for word in tokens if word not in stopwords] # removing stopwords\n",
    "                    words=set(words)                                         # taking only unique words\n",
    "                    vector=text_set.intersection(words)                      # checking for intersection with entered text \n",
    "                    vectors.append(len(vector))                              # appending value to vectors list\n",
    "                    \n",
    "                vectors=sorted(list(enumerate(vectors)), key= lambda x: x[1], reverse=True)[:num] # sorting the list in desc\n",
    "                indices= [i[0] for i in vectors]                                         # selecting indecies of top values\n",
    "                print(f\"Top {num} Best Resturants Based on entered text:\")\n",
    "                # using the indices fileter the dataframe \n",
    "                df=df.loc[indices].sort_values(by=['b/s_rating','review_count'],ascending=False)\n",
    "                if location: folium_map(df)                                   # calling the folim_map of the selected values\n",
    "                return df[['name','b/s_rating','review_count','location']].reset_index(drop=True)    # offering recommendatons\n",
    "        \n",
    "        # the if only location is entered as a parameter then the top businesses in that location are recommended\n",
    "        if location:\n",
    "            df=df.loc[ df.location.str.contains(location)& (df['b/s_rating']>=rating)] #filtering dataframe\n",
    "            df=df.sort_values(['review_count','b/s_rating'])[:num]     # sorting in descending order\n",
    "            folium_map(data=df)\n",
    "            return df[['name','b/s_rating','review_count','location']].reset_index(drop=True) # offering recommendations\n",
    "         \n",
    "        # if both the name, text and location are None the most popular restarants are recommended\n",
    "        else:                \n",
    "            df=df.loc[data['b/s_rating']>=rating].sort_values(by=['review_count','b/s_rating'],ascending=False)[:num]\n",
    "            if location: folium_map(data=df)\n",
    "            print(\"Most Popular Restaurants\")\n",
    "            return df[['name','b/s_rating','review_count','location']].reset_index(drop=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c12469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running the recommender on default parameters\n",
    "content_based()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2ad5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offering recommndations based on a specific location\n",
    "content_based(location='Philadelphia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf07b65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommending similar restaurants to the entered name\n",
    "content_based( name=\"Backspace Bar & Kitchen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f93683c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommending restaurants with attributes in the entered text\n",
    "content_based( text=\"I want a clean restaurant with good desert and parking space and a romantic environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34826091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recommending restaurants with attributes in the entered text\n",
    "content_based(rating=4, location=\"PA\",num=10,\\\n",
    "        text=\"restaurant with crubs and sushi and a romantic setting and parking space. also the food should be delicious\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490930bc",
   "metadata": {},
   "source": [
    "#### Collaborative filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01297211",
   "metadata": {},
   "source": [
    "Here the tasks related to building a collaborative filtering recommendation system using the Surprise library are undertaken for collaborative filtering by selecting the relevant columns, importing the Surprise library, initializing a Reader object to specify the data format, and then loading the data into a Surprise Dataset object for further analysis and model building.\n",
    "\n",
    "> Now, we will compare the different neighborhood-based models and see which ones perform best based on the RMSE metric, the afterwards compare the based neighborhood-based model with the model-based models and pick the best model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fe5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting specific columns that are relevant for collaborative filtering\n",
    "new_df = data[['user_id', 'business_id', 'rating']] \n",
    "reader = Reader()\n",
    "data_2 = Dataset.load_from_df(new_df,reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3752f52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = data_2.build_full_trainset()\n",
    "print('Number of users: ', dataset.n_users, '\\n')\n",
    "print('Number of Restaurants: ', dataset.n_items)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f04232",
   "metadata": {},
   "source": [
    "> Fisrt , we will model a baseline SVD() model using the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573beed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantating the SVD model\n",
    "svd = SVD()\n",
    "\n",
    "# using cross-validate to get the test rmse scores for 5 splits\n",
    "results=cross_validate(svd, data_2, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "for values in results.items():\n",
    "    print(values)\n",
    "print(\"-------------------------\")\n",
    "print(\"Mean RMSE: \",results['test_rmse'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b10d04c",
   "metadata": {},
   "source": [
    "The first baseline model-based model with an RMSE of 1.256 same as our best neighborhood based model which had a RMSE of 1.257. Using the GridSearchCv we will tune the SVD model inorder to improve the training RMSE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9e7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a dictionary params with hyperparameter values to be tested\n",
    "params = {'n_factors': [20, 50, 100], # number of factors for matrix factorization\n",
    "         'reg_all': [0.02, 0.05, 0.1]} # regularization term\n",
    "# create a GridSearchCV object 'g_s_svd' for hyperparameter tuning\n",
    "g_s_svd = GridSearchCV(SVD,param_grid=params,n_jobs=-1) # specify the algorithm (SVD) to be tuned\n",
    "# fit the GridSearchCV object to the data to find the best hyperparameters\n",
    "g_s_svd.fit(data_2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40547c5c",
   "metadata": {},
   "source": [
    "Here we perform hyperparameter tuning for the SVD collaborative filtering model using grid search and cross-validation. It tests different values of the number of latent factors (n_factors) and the regularization term (reg_all) to find the combination that results in the best model performance. The final best hyperparameters can be accessed from the g_s_svd object for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1336a50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_s_svd.best_score)\n",
    "print(g_s_svd.best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4de9c",
   "metadata": {},
   "source": [
    "The RMSE value for the optimized SVD model is approximately 1.25, indicating the model's average prediction error in terms of user ratings. Lower RMSE values are desirable as they signify better predictive accuracy.                              \n",
    "The MAE value for the optimized SVD model is approximately 1.01, representing the average absolute difference between predicted and actual user ratings. A lower MAE indicates improved prediction accuracy.                                            \n",
    "The best-performing hyperparameter values are as follows:                       \n",
    "1) For RMSE, the optimal hyperparameters are 'n_factors' = 20 and 'reg_all' = 0.05.\n",
    "2) For MAE, the optimal hyperparameters are 'n_factors' = 20 and 'reg_all' = 0.02.   \n",
    "These results indicate that the SVD collaborative filtering model, when configured with these hyperparameters, provides a relatively low prediction error and is well-suited for making personalized recommendations based on user ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21390798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# created an instance of the SVD model with specified hyperparameters\n",
    "svd = SVD(n_factors= 20, reg_all=0.02)\n",
    "# fit the SVD model to the dataset\n",
    "svd.fit(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71be06c1",
   "metadata": {},
   "source": [
    "The code we just did initializes an SVD model with specific hyperparameters and then trains the model on the provided dataset. The trained SVD model can be used for various tasks, such as making personalized recommendations based on user-item interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a667c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the model them we try and make a rating prediction of user 15, on restaurant with id \"Pns2l4eNsfO8kk83dixA6A\"\n",
    "svd.predict(\"15\", \"Pns2l4eNsfO8kk83dixA6A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe1dfb8",
   "metadata": {},
   "source": [
    "> First before creating a collaborative filtering function, we will first create a funtion **restaurant_rater()** that suggest resturants to users for them to input their rating inorder based on the entered rating to offer recommendations using the SVD model since it cannot offer recommendations when the user has no data in the database (cold start problem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbf13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function named 'restaurant_rater' that takes user inputs to rate restaurants\n",
    "\n",
    "def restaurant_rater(data=df,num:int=3, location:str =None, category:str =None):\n",
    "    \n",
    "    \"\"\"\n",
    "    The functions takes the following inputs:\n",
    "    data: DataFrame - a dataframe containing only rows of the unique business \n",
    "    num: int - number of ratings\n",
    "    location: string - prefered location\n",
    "    category: string - prefered category of restaurant\n",
    "    \n",
    "    Then randomly draws a restaurant names from the dataframe for the user to rate\n",
    "    \"\"\"\n",
    "    \n",
    "    df_restaurant=data\n",
    "    #assigning the rating user a user_id\n",
    "    user_id=df_restaurant.user_id.max()+1                                               \n",
    "\n",
    "    rating_df = pd.DataFrame()   # create an empty dataframe to store user rated restaurants\n",
    "    # continue the loop until the desired num is collected\n",
    "    while num > 0:\n",
    "        # select a random restaurant that matches the specified location\n",
    "        if location: \n",
    "            restaurant = df_restaurant[df_restaurant['location'].str.contains(location)].sample(1)\n",
    "         # select a random restaurant that matches the specified category    \n",
    "        elif category:\n",
    "            restaurant = df_restaurant[df_restaurant['categories'].str.contains(category)].sample(1)\n",
    "        else:  # or else selects a random restaurant\n",
    "            restaurant = df_restaurant.sample(1)\n",
    "        # prints the selected restaurant    \n",
    "        print(tabulate(restaurant[['name','b/s_rating','categories']], headers='keys', tablefmt='fancy_grid', showindex=False))\n",
    "        # asks for rating from user\n",
    "        rating = input(\"How do you rate this restaurant on a scale of 1-5, Enter: \")\n",
    "        \n",
    "        # creating a function that checkes the validity of the entered rating ie should be bewteen 1-5\n",
    "        def checker(rating):\n",
    "            if (len(rating)!= 0):\n",
    "                while (float(rating)>5) :\n",
    "                    print(\"Enter valid rating, scale of 1-5 or Enter\")  \n",
    "                    rating= input()\n",
    "                return rating\n",
    "            else: return rating\n",
    "            \n",
    "        # calling the function to confirm the selected rating \n",
    "        rating = checker(rating)\n",
    "        if len(rating) == 0:                                        # if no rating is entered \n",
    "            num-=1                                                  # the jumps to select another restaurant\n",
    "            continue\n",
    "        else:\n",
    "            restaurant.loc[:,('user_id')]= user_id                   # then the selected restaurant is assigned the user id\n",
    "            restaurant.loc[:,('rating')]= rating\n",
    "            rating_df=pd.concat([rating_df,restaurant], axis=0)   # the movie is added to our new user rated dataframe\n",
    "            num-=1                                                  # then another restaurant is suggested till num==0\n",
    "            # return the list of user ratings and restaurant information\n",
    "    return rating_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fd6ddc",
   "metadata": {},
   "source": [
    "This function above allows a user to interactively rate restaurants by providing their ratings for a specified number of restaurants, and it collects this information in a list for further analysis or use in a recommendation system. The code also considers the restaurant category for selecting restaurants to rate if a category is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7954aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating 4  restaurants that have sbadwiches in their menu\n",
    "restaurant_rater( num=4, category='Sandwiches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2002d7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rating ratsurants from PA\n",
    "restaurant_rater(location='PA')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca050e",
   "metadata": {},
   "source": [
    "> With the defined rated() function, we proceed to create a collaborative filtering function  while using our SVD model. The the function when the user has not entered any  rating to the suggested rsetaurants (cold start problem). Then the function will recommend using the content-based system, solving the cold start problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cf_model(df=df,num:int=3, location:str=None, name=None , text=None):\n",
    "    \"\"\"\n",
    "    The function takes the following inputs;\n",
    "    \n",
    "    df: DataFrame - a dataframe containing unique rseturants\n",
    "    name: str - name of restaurant to recomend similar restaurants\n",
    "    num:int - number of restaurants to recommend\n",
    "    location: string - prefered location\n",
    "    category: string - prefered category of restaurant\n",
    "    text: - User prefrencese inform of text\n",
    "    \n",
    "    The function the takes user ratings and appends then to the dataframe then fits this new dataframe to the SVD model\n",
    "    Then predicts this users rating on all the restaurants in the dataframe then selects the top rating predictions and \n",
    "    recomends those restaurants\n",
    "    \"\"\"\n",
    "    \n",
    "    # calling the rater function for user to enter restaurant ratings\n",
    "    user_ratings=restaurant_rater(num=num, location=location)\n",
    "    \n",
    "    # when the user ratings come back being blank, ie no ratings given ie cold start problem\n",
    "    # then the content-based method is called, which then makes the recommendations\n",
    "    if len(user_ratings)==0:\n",
    "        return content_based(df=df,num=num,name=name,location=location,text=text,)\n",
    "    \n",
    "    # then add the user ratings to our df\n",
    "    df=pd.concat([df,user_ratings],axis=0)\n",
    "\n",
    "    # convert the new dataset into surprise format\n",
    "    dataset = Dataset.load_from_df(df[['user_id','business_id','rating']],reader)\n",
    "    \n",
    "    # then fit the surprise data to the SVD\n",
    "    svd = SVD(n_factors= 20, reg_all=0.02)\n",
    "    svd.fit(dataset.build_full_trainset())\n",
    "    \n",
    "    # extract the user rating in the ratings dataframe\n",
    "    user_id=user_ratings['user_id'].values[0]\n",
    "    \n",
    "    # select a random restaurant that matches the specified category\n",
    "    if location: \n",
    "            df = df.loc[df['location'].str.contains(location)]\n",
    "        \n",
    "    #create an empty list to append the model predictions\n",
    "    user_predictions=[]\n",
    "    \n",
    "    # loopin over all the unique restaurant ids in the dataframe and appending the predictions to user prediction list\n",
    "    for  iid in df.business_id.unique():\n",
    "        user_predictions.append( (iid , svd.predict(user_id, iid)[3]))\n",
    "    \n",
    "    # sorting the predictions in descending order of the predictions values\n",
    "    top_pred = sorted(user_predictions , key =lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # selecting the top 'num'(number of predictions) prediction indicies\n",
    "    indices=[i[0] for i in top_pred[:num]]  \n",
    "    \n",
    "    #using the extracted indices them extract the resturant titles\n",
    "    rec = df.loc[ df['business_id'].isin(indices)].sort_values('b/s_rating', ascending=False)\n",
    "    display(folium_map(rec))\n",
    "    \n",
    "    #then retun the resturants details to user\n",
    "    return rec[['name','b/s_rating','location']].reset_index(drop=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9af989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on default parameters\n",
    "cf_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2353c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offering recommenations based on a sepecified location\n",
    "\n",
    "cf_model(num=5,location='LA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f9b172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the cf model to offer content based recommendations\n",
    "# and not inputing any ratings on the suggected restaurants\n",
    "cf_model(location='New Orleans',text=\"restaurant with delicious crubs and nice outdoor setting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3ffef0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In conclusion, this project has successfully achieved its main objective of developing an interactive and user-friendly restaurant recommendation system. This system not only provides personalized dining suggestions but also takes into account various factors that influence restaurant ratings and user preferences. The integration of an advanced recommendation algorithm ensures that users can access tailored restaurant recommendations, enhancing their overall dining experiences.\n",
    "\n",
    "Throughout this project, we also met specific objectives. We designed and developed a user-friendly website, making it convenient for users to interact with our recommendation system. Additionally, we conducted in-depth analyses of the factors that significantly impact restaurant ratings and user preferences. This understanding was crucial in refining our recommendation algorithms, ensuring they provide valuable and relevant suggestions to users.\n",
    "\n",
    "Furthermore, we harnessed the power of geographical data visualization using Folium. By creating interactive maps, we were able to explore geographic trends related to restaurant recommendations. These maps not only make our recommendations more engaging but also help users discover new dining experiences in their preferred locations.\n",
    "\n",
    "In summary, this project's multifaceted approach aimed at delivering a holistic restaurant recommendation system has proven successful. Users can now access personalized dining recommendations, taking into account various influencing factors and geographical trends. This project not only achieves its specific objectives but also offers a valuable service that enhances the dining experiences of users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82302fda",
   "metadata": {},
   "source": [
    "## Recommendations \n",
    "\n",
    "Integration of user feedback: Actively seek and integrate user feedback to refine and improve the recommendation system\n",
    "\n",
    "Enhanced user profiles: Use this data to provide more personalized restaurant recommendations.\n",
    "\n",
    "Enhance recommendation algorithms: Continue to refine and enhance the recommendation algorithms. Explore more advanced machine learning techniques, including deep learning, to improve recommendation accuracy and personalization.\n",
    "\n",
    "Expand geographical coverage: Gradually expand the geographical coverage of the recommendation system to include more regions and cities, providing users with a broader range of dining options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd39a5",
   "metadata": {},
   "source": [
    "## Future Improvement Ideas\n",
    "\n",
    "Enhanced visuals: Incorporate images and visual content, such as restaurant photos and dishes, to make the recommendations more visually appealing and informative.\n",
    "\n",
    "Community and social sharing: Encourage users to share their dining experiences and reviews within the system. Implement social sharing features to build a user community and facilitate restaurant recommendations from peers.\n",
    "\n",
    "Real-time updates: Enable real-time updates of restaurant information, including opening hours, special offers, and menu changes. Users should receive the most current and accurate data.\n",
    "\n",
    "Integration with food delivery services: Collaborate with food delivery services to allow users to place food orders for delivery or pickup directly through the recommendation system.\n",
    "\n",
    "Advanced machine learning algorithms: Explore the use of advanced machine learning algorithms to further enhance recommendation accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5956275b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df33c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
